So i thought I made something like this before but I'm not seeing it.

I think, however, I can use 
https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation-iterators-for-grouped-data
Groups, and threshold to create a group.

The most test like group

So it sounds like the first thing to do is to train an estimator that can predictt
the liklihood of something being in the test set or not

Then given a data set, threshold data
return https://scikit-learn.org/stable/modules/cross_validation.html#predefined-fold-splits-validation-sets
predefined split with 0 for the validation set dervied from the dataset
see: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.PredefinedSplit.html#sklearn.model_selection.PredefinedSplit

Then we have a cross validator that can be used, reused based on threshold.

should sketch out responsibilties and then decompose with SOLID
https://scotch.io/bar-talk/s-o-l-i-d-the-first-five-principles-of-object-oriented-design
--

so I realized later that I could instead break up seperation of concerns and do the following:
* make a class that accepts a classifier, predicts probability and recieves a dataset, 
with test examples marked. On fit it learns to predict if an example if test or not
* make a scorer that accepts a y predicted, y actual and probability of being test instance, returns a scorer

the issue is thst the scorer does not accept other data

estimators have a scoring function, https://github.com/scikit-learn/scikit-learn/blob/1495f6924/sklearn/model_selection/_search.py#L829#L849

let's see if they have access to X (they must?)

okay, see: https://github.com/scikit-learn/scikit-learn/blob/1495f6924/sklearn/base.py#L332
looks like the cleanest solution is to make a scorer for a estimator mixin and 
inheriet the base estimator. From there all CV, grid,etc soln shoudl work
--
So I should probably make a Transformer that learns to label test, train instances
pass the transformer as a mixin to classifer mixin, which also takes a custom scorer. 
I'm nervous that the scorer depends on the transformer
also see: https://github.com/heykarimoff/solid.python

so, from the above the scorer should have an abstract interface to the transformer, ah but it 
might not use the scorer except for the predict function, which violates seperation

I think you can use a mixin, letme double check aginast SRP
is interesting: https://punq.readthedocs.io/en/latest/
--

okay so I think the cleanest way to do this is start with the Transformer

-- 

have util functions working, verified the basic structure of the Adversarial labeller works

The AdversarialLabeller, using LR mixin, doesn't seem to also have .solver
like LR does.

--

Okay, been a while before I put notes back in htere.
I've gotten things set up mostly better, I confused myself with the wrong target
variable. But ther ear some fixes that are needed:

My sample weights are not normalized to add up to one, that needs to happen. 
I am not sure if it'll affect how the random forest weights its learning though.

I'm seeing a roughly 10 point different between my hold out and Kaggle's hold out
Yet I'm only getting AUC for predicting if a sample is in test or not

The examples I've seen of adversarial testing 
take those scored as 1 only: https://www.kaggle.com/kevinbonnes/adversarial-validation

I am curious about modifying scoring while training 
    like if predicted 1, then weight as 1
    else weight as 0.5

```
    mask  = sample_weights == 1
    print(
        accuracy_score(labels[rf_test_df.index][mask].values,
                       rf2.predict(rf_test_df[mask].values)
        )
    )
```

GIves 0.68 on hold out, which is a bit closer to 0.64
---
Trained with test label detecor AUC of 0.56, however, hold out score is 0.82
actual score is 0.60 ... so the extra trees memorized the test distribution a tad too much

There is something slightly different, which might affect things:
* in practice they mark the test examples as 1 and all others as 0
* They then train on the whole thing, use CV validation, they do not randomly sample
like I have. 
* Their AUC to validated result vs actual is much closer

todo: do straight full data set training, see if that makes the actual vs validated
results much closer. This will allow us to then make a scorer
that can do validated scoring and drive model parameter selection from the 
fold, etc.
--

rewrote some parts, focused on cross validation, saw a lot of variation.
have an issue with classifiers want to predict 1 a lot. Currently actual
is target with 0.68 precision, almost full recall, on hold out set.

going to try with only balanced, balanced subsample approaches
that works a bit better:

Validation Accuracy: 0.61
              precision    recall  f1-score   support

           0       0.33      0.19      0.24       363
           1       0.67      0.81      0.74       750

    accuracy                           0.61      1113
   macro avg       0.50      0.50      0.49      1113
weighted avg       0.56      0.61      0.57      1113

note: https://github.com/scikit-learn-contrib/imbalanced-learn
note: https://arxiv.org/abs/1804.07155

using a lower ntrees start, entropy

Accuracy: 0.61 (+/- 0.07)
Validation Accuracy: 0.61
              precision    recall  f1-score   support

           0       0.33      0.19      0.24       363
           1       0.68      0.82      0.74       750

    accuracy                           0.61      1113
   macro avg       0.51      0.50      0.49      1113
weighted avg       0.56      0.61      0.58      1113

kNN GIves
Accuracy: 0.61 (+/- 0.07)
Validation Accuracy: 0.66
              precision    recall  f1-score   support

           0       0.37      0.06      0.11       363
           1       0.68      0.95      0.79       750

    accuracy                           0.66      1113
   macro avg       0.52      0.51      0.45      1113
weighted avg       0.58      0.66      0.57      1113

the arxiv paper above inspired me to try kNN
