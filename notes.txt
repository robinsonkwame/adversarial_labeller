So i thought I made something like this before but I'm not seeing it.

I think, however, I can use 
https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation-iterators-for-grouped-data
Groups, and threshold to create a group.

The most test like group

So it sounds like the first thing to do is to train an estimator that can predictt
the liklihood of something being in the test set or not

Then given a data set, threshold data
return https://scikit-learn.org/stable/modules/cross_validation.html#predefined-fold-splits-validation-sets
predefined split with 0 for the validation set dervied from the dataset
see: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.PredefinedSplit.html#sklearn.model_selection.PredefinedSplit

Then we have a cross validator that can be used, reused based on threshold.

should sketch out responsibilties and then decompose with SOLID
https://scotch.io/bar-talk/s-o-l-i-d-the-first-five-principles-of-object-oriented-design
--

so I realized later that I could instead break up seperation of concerns and do the following:
* make a class that accepts a classifier, predicts probability and recieves a dataset, 
with test examples marked. On fit it learns to predict if an example if test or not
* make a scorer that accepts a y predicted, y actual and probability of being test instance, returns a scorer

the issue is thst the scorer does not accept other data

estimators have a scoring function, https://github.com/scikit-learn/scikit-learn/blob/1495f6924/sklearn/model_selection/_search.py#L829#L849

let's see if they have access to X (they must?)

okay, see: https://github.com/scikit-learn/scikit-learn/blob/1495f6924/sklearn/base.py#L332
looks like the cleanest solution is to make a scorer for a estimator mixin and 
inheriet the base estimator. From there all CV, grid,etc soln shoudl work
--
So I should probably make a Transformer that learns to label test, train instances
pass the transformer as a mixin to classifer mixin, which also takes a custom scorer. 
I'm nervous that the scorer depends on the transformer
also see: https://github.com/heykarimoff/solid.python

so, from the above the scorer should have an abstract interface to the transformer, ah but it 
might not use the scorer except for the predict function, which violates seperation

I think you can use a mixin, letme double check aginast SRP
is interesting: https://punq.readthedocs.io/en/latest/
--

okay so I think the cleanest way to do this is start with the Transformer

-- 

have util functions working, verified the basic structure of the Adversarial labeller works

The AdversarialLabeller, using LR mixin, doesn't seem to also have .solver
like LR does.

--

Okay, been a while before I put notes back in htere.
I've gotten things set up mostly better, I confused myself with the wrong target
variable. But ther ear some fixes that are needed:

My sample weights are not normalized to add up to one, that needs to happen. 
I am not sure if it'll affect how the random forest weights its learning though.

I'm seeing a roughly 10 point different between my hold out and Kaggle's hold out
Yet I'm only getting AUC for predicting if a sample is in test or not

The examples I've seen of adversarial testing 
take those scored as 1 only: https://www.kaggle.com/kevinbonnes/adversarial-validation

I am curious about modifying scoring while training 
    like if predicted 1, then weight as 1
    else weight as 0.5

```
    mask  = sample_weights == 1
    print(
        accuracy_score(labels[rf_test_df.index][mask].values,
                       rf2.predict(rf_test_df[mask].values)
        )
    )
```

GIves 0.68 on hold out, which is a bit closer to 0.64
---
Trained with test label detecor AUC of 0.56, however, hold out score is 0.82
actual score is 0.60 ... so the extra trees memorized the test distribution a tad too much

There is something slightly different, which might affect things:
* in practice they mark the test examples as 1 and all others as 0
* They then train on the whole thing, use CV validation, they do not randomly sample
like I have. 
* Their AUC to validated result vs actual is much closer

todo: do straight full data set training, see if that makes the actual vs validated
results much closer. This will allow us to then make a scorer
that can do validated scoring and drive model parameter selection from the 
fold, etc.
--

rewrote some parts, focused on cross validation, saw a lot of variation.
have an issue with classifiers want to predict 1 a lot. Currently actual
is target with 0.68 precision, almost full recall, on hold out set.

going to try with only balanced, balanced subsample approaches
that works a bit better:

Validation Accuracy: 0.61
              precision    recall  f1-score   support

           0       0.33      0.19      0.24       363
           1       0.67      0.81      0.74       750

    accuracy                           0.61      1113
   macro avg       0.50      0.50      0.49      1113
weighted avg       0.56      0.61      0.57      1113

note: https://github.com/scikit-learn-contrib/imbalanced-learn
note: https://arxiv.org/abs/1804.07155

using a lower ntrees start, entropy

Accuracy: 0.61 (+/- 0.07)
Validation Accuracy: 0.61
              precision    recall  f1-score   support

           0       0.33      0.19      0.24       363
           1       0.68      0.82      0.74       750

    accuracy                           0.61      1113
   macro avg       0.51      0.50      0.49      1113
weighted avg       0.56      0.61      0.58      1113

kNN GIves
Accuracy: 0.61 (+/- 0.07)
Validation Accuracy: 0.66
              precision    recall  f1-score   support

           0       0.37      0.06      0.11       363
           1       0.68      0.95      0.79       750
    accuracy                           0.66      1113
   macro avg       0.52      0.51      0.45      1113
weighted avg       0.58      0.66      0.57      1113

the arxiv paper above inspired me to try kNN
---
imported imbalnced sklearn
with balanced random forest we getting
Validation Accuracy: 0.51
              precision    recall  f1-score   support

           0       0.34      0.56      0.43       363
           1       0.69      0.49      0.57       750

    accuracy                           0.51      1113
   macro avg       0.52      0.52      0.50      1113
weighted avg       0.58      0.51      0.52      1113

(note the Accuracy scores above were printing old score info, hence being the same; i think)

okay, so what I really want is to essentially make a labeller that is a pretty 
much always correct when the predicted probabilty is a 1. I don't particularly wnat to
muck around too much so I can, instead, attempt to maximize the F1 score of both
in test and out of test predictions.

In a sense, because imbalance respects class imbalance, I can probably use
maximizing the in test class F1 score and it won't go off the rails and say
everythign is a 1 (gettin ga accuracy of 0.67 but 0 for out of class)

so What I should do here to close out this fork of work is make an parameter _search
imbalanced RUSBoost classifier and leave it at that.

The final step will be to add more text features and then, finally test against Kaggle
results from a holdout validation step
--

okay do a full rusboost run (that was too long), gotten

Validation Accuracy: 0.34
              precision    recall  f1-score   support

           0       0.32      0.90      0.47       363
           1       0.58      0.07      0.12       750

    accuracy                           0.34      1113
   macro avg       0.45      0.48      0.29      1113
weighted avg       0.50      0.34      0.23      1113

Given that that this is a binary problem, it would seem that I could flip the deicsions and get 
a validation accuracy of 0.66?

Yeah, this is funny:

In [93]:     print( 
    ...:         "Validation Accuracy: %0.2f" % ( 
    ...:             accuracy_score( 
    ...:                 labels[test_df.index], 
    ...:                 adversarial_labeller.predict( 
    ...:                     test_df.values 
    ...:                 ) ^ 1 
    ...:             ) 
    ...:         ) 
    ...:     )                                                                          
Validation Accuracy: 0.66
(note the ^ 1 for bit flipping)
and

In [94]:     print( 
    ...:         classification_report( 
    ...:             y_true= labels[test_df.index], 
    ...:             y_pred= adversarial_labeller.predict( 
    ...:                         test_df.values 
    ...:                     ) ^ 1 
    ...:         ) 
    ...:     )                                                                          
              precision    recall  f1-score   support

           0       0.42      0.10      0.16       363
           1       0.68      0.93      0.79       750

    accuracy                           0.66      1113
   macro avg       0.55      0.52      0.47      1113
weighted avg       0.59      0.66      0.58      1113

k, I've taken this about as far as I should go. 

I think at this point I'm at the essential question (and probably over fitting
to one data set) of whether for adversarial validation is better having
A) a higher accuracy (validating against those prediced to be 1) or
B) a higher predicted probably (e.g. prob = 1)

I think the answer is strongly influenced by the data drift present since (A) 
brings in non test like samples for validation. The score could also be weighted by
sample confidence but the RUSBooster seems to give one of two confidences and that's it.

okay so what I shoudl do is wrap this part up

The rusbooster appears to be best, ignoring the predicited probability

Okay, I think I'm done at this part, just need to capture the code in main() 
and make into a function to build out the labeller. Actually I think I'm done,
running fit and then maiximize adversarl validation seperates concerns and isn'tad
a big lift

okay so the next steps next week (if I don't abandon this work due to a wave of new 
class work) is to:
* holdout some data
* train a normal random forest on the remaining data
* label the hold out as in test or out of test
* get teh accuracy score on the in test hold out data using the RF and known label
* that score should be +/- 3 points within the Kaggle result (ideally)

If that doesn't work then that might suggest that the predict probabilities are more important
(although I feel liek might've failed way back when?), then figure out an approaches
that maximizes predicted probabilty as well as gets punished for predicting all 1s
--

okay, so in a way I have an inital success
adverarial labelling predicts 0.67
Kaggle gave 0.66985

The thing is my adverarial labeller, in this case, was trained with a minimal set of checking to speed
up dev time. What I watn to do is switch to a random CV, get a good version and try again.

the other thing that bothers me is how complex this is to set up, i kinda wnat a one function call
you give it the data, a classifier and it'll return an adversarial validation score.

but I defintely got where i wanted today, need to test against a well cross validated parameter set
and verify it still works as expected.

then i should try it against a different type of classifier and then rearchitect slightly for ease of use
we can already drop it into sklearn fairly easily.
--

So I did some deeper searching, around sklearn pipelines, and it turns out there's a
cross validator that accepts a scoring function, here I would not use the make_scorer
see sklearn 3.3.1.3: Implementing your own scoring object.

This is what teh adversial labeller needs to be written as, then labels X
and only evaluates on those X labelled as test.
--

so the adverarial part would need it's own transformer I think (or none)
it actaully may need a mini transform pipeline
so I think I would actually train the a...

okay, so you would make a factory of sorts that trans, generates
a pipeline that ends in the adversarial labeller and can be dropped in
the factory needs the rigth input, any pipeline components as well and then
append the trained adversarial predictor to the end of it. Kinda annoying
but this will make it drop in easy.
---
okay, got basic inital for factory, let's commit
--
so now I want to get the get_best_paramet working, then make it actually return a fitted
adverarial labeller (that calls the pipeline on it too)
-

So the pipeline object works, needs to be called with
In [89]:     labelled_test_mask =\ 
    ...:         fit.predict( 
    ...:             data['validate']['data'].drop("PassengerId", axis="columns") 
    ...:         ) == 1    
   
which is kinda weird but it's a function of how I set up the preprocessor
and what data I fed it

with the above we get
In [93]:     # and use that test labeled set as a validation set, compare to Kaggle 
    ...:     accuracy_score( 
    ...:         y_true= data["validate"]["labels"][labelled_test_mask].values, 
    ...:         y_pred= clf.predict( 
    ...:           data["validate"]["data"][labelled_test_mask].values 
    ...:         ) 
    ...:     ) 
    ...:                                                                                                                                  
Out[93]: 0.6940298507462687

and I noticed that it says all labels are true on the hold out set
and we already predicted against that and got 0.67, so I will skip that
verification for now (?)

Goign to callit, next steps are to:

* test against / add in scorer for custom cross validation scorer,
see: https://scikit-learn.org/stable/modules/model_evaluation.html#implementing-your-own-scoring-object
for gory details

* rewrite main, the example file to use the piepeline, provide 
fixture specific test files (should make this a test I suppose), move
to unit test, but add as to read me as a simple example

* verify adversarial accuracy is mostly spot on (w/in -/+ 1 point of accuracy)

* modify so it's pip installable
--
okay so have in test the class that produces a labeller
I now need to read in data, pass as a scorer to a 
CV parameter fit.
--
have a sketch for the cross val, as a scorer
I'm seeing different validation results, which all should
be the same, I suspect the randomizedCV search is starting
along different paths, i thought i passed in the same seed value.
need to double check this

Scorer was also failing, i forget the exact message but I removed teh 
wrapper (which was in the example) and it seems to work. I now
see an issue with one of transformers that expects a data frame,
we now see:
---
  File "/Users/kimrobinson/.local/share/virtualenvs/adversarial_validatior-jlSiO2el/lib/python3.7/site-packages/imblearn/pipeline.py", line 347, in predict
    Xt = transform.transform(Xt)
  File "/Users/kimrobinson/.local/share/virtualenvs/adversarial_validatior-jlSiO2el/lib/python3.7/site-packages/sklearn/preprocessing/_function_transformer.py", line 153, in transform
    return self._transform(X, func=self.func, kw_args=self.kw_args)
  File "/Users/kimrobinson/.local/share/virtualenvs/adversarial_validatior-jlSiO2el/lib/python3.7/site-packages/sklearn/preprocessing/_function_transformer.py", line 179, in _transform
    return func(X, **(kw_args if kw_args else {}))
  File "/Users/kimrobinson/Desktop/Kwame/open_source/adversarial_validatior/tests/test_adversarial_labeller.py", line 45, in keep_numeric_columns_only_replace_na
    return df.select_dtypes(include='number')\
AttributeError: 'numpy.ndarray' object has no attribute 'select_dtypes'
---

which suggests that I update the transformer to take in a numpy matrix instead
so that it'll work on both. I can do this with a wrapper inside teh function

--
added some code to convert a numpy matrix to pandas dataframe, drop keep_numeric_columns_only_replace_na

note when intallign in a new directory you must run
`pipenv shell && pipenv install && pipenv install --dev`
since --dev has pytest, otherwise it'll call the system version and try to run yourp
python3 (e.g. python3.5) and you wil lget f string format related errors since it doesn't support f strings

```
(adversarial_validator) kwame@Puget-168695:$ pytest --version
This is pytest version 3.5.0, imported from /home/kwame/.local/lib/python3.5/site-packages/pytest.py
(adversarial_validator) kwame@Puget-168695:$ python pytest --version
python: can't open file 'pytest': [Errno 2] No such file or directory
(adversarial_validator) kwame@Puget-168695:$ python -m pytest --version
This is pytest version 5.2.0, imported from /home/kwame/.local/share/virtualenvs/adversarial_validator-OGseoHRw/lib/python3.7/site-packages/pytest.py
```

okay, then you hae to run with `python -m pytest --pdb -s` >:|
-
Got the labeller workign int he cv loop, I can't trigger the prediction failure
but the validation accuracy results have differnt 0, 1 precisions so I know
that the RandomizedSearchCV or somethign is different somethign idfferent
but i also pass in a 1 for random state so I'm not where this is coming from.
--
Okay so next steps are to include an example with fake data in the README
Make it pip installable

for RandomizedSearchCV, see this note
```
Note that before SciPy 0.16, the scipy.stats.distributions do not
accept a custom RNG instance and always use the singleton RNG from
numpy.random. Hence setting random_state will not guarantee a deterministic
iteration whenever scipy.stats distributions are used to define the parameter
search space
```

but

```
Python 3.7.2 (default, Jan 11 2019, 21:31:15) 
[GCC 5.4.0 20160609] on linux
Type "help", "copyright", "credits" or "license" for more information.
>>> import scipy
>>> scipy.__version__
'1.3.1'
```
so not sure what's up there
---
x okay, need to test out pip installable
got somethign basic up, need to remember to update when done with the
other stuff

hmm, so I'm almost done, I need to write a quite example of use
so i think i can make data drfit by generating a two features classification 
problem w sklearn and then shift the mean slightly and mark those test

okay, i got to go to bed here but:
a) use sample_generator to create read_concat_and_label_test_train_data, with a
train and test set, where test set is train set + 1 sd or something <-- data drift
mark with 0 or 1 as appropriate. hold out the test set answers
b) Call get variables and labels, pass to factory for adver. labeller
c) create a classifer
d)  make a split of training with a portion reserved for test
  classifer, labeller pipeline
    to cross_val_score

    labeller pipeline should pick from those in test that look like
    the real test

    get overall score (too optimistic)
    get adversarial labelled score

    compare to true test, should be similar
--
scratch

import numpy as np
import pandas as pd
from sklearn.datasets.samples_generator import make_blobs
from sklearn.metrics import accuracy_score
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestClassifier
from adversarial_labeller import AdversarialLabelerFactory, Scorer

# Our blob data generation parameters for this example
number_of_samples = 1000
number_of_test_samples = 300

# Generate 1d blob data and label a portion as test data
# ... 1d blob data can be visualized as a rug plot
variables, labels = \
  make_blobs(
    n_samples=number_of_samples,
    centers=2,
    n_features=1,
    random_state=0
)

df = pd.DataFrame(
  {
    'independent_variable':variables.flatten(),
    'dependent_variable': labels,
    'label': 0  #  default to train data
  }
)
test_indices = df.index[-number_of_test_samples:]
train_indices = df.index[:-number_of_test_samples]

df.loc[test_indices,'label'] = 1  # ... now we mark instances that are test data

# Now perturb the test samples to simulate data drift/different test distribution
df.loc[test_indices, "independent_variable"] +=\
  np.std(df.independent_variable)

# ... now we have an example of data drift that adversarial labeling 
# can be used for this example

features_for_labeller = df.independent_variable
labels_for_labeller = df.label

pipeline, flip_binary_predictions =\
    AdversarialLabelerFactory(
        features = features_for_labeller,
        labels = labels_for_labeller,
        run_pipeline = False
    ).fit_with_best_params()

scorer = Scorer(the_scorer=pipeline,
                flip_binary_predictions=flip_binary_predictions)

# Now we evaluate a classifer on training data only, but using
# our fancy adversarial labeller

# ... sklearn wants firmly defined shapes
clf_adver = RandomForestClassifier(n_estimators=100, random_state=1)
_X = df.loc[train_indices]\
       .independent_variable\
       .values\
       .reshape(-1,1)
adversarial_scores =\
    cross_val_score(
        X=_X,
        y=df.loc[train_indices].dependent_variable,
        estimator=clf_adver,
        scoring=scorer.grade,
        cv=5,
        n_jobs=1,
        verbose=2)

# ... and we get ~ xyz
average_adversarial_score =\
    np.array(adversarial_scores).mean()

# ... let's see how this compares with normal cross validation
clf = RandomForestClassifier(n_estimators=100, random_state=1)
scores =\
    cross_val_score(
        X=_X,
        y=df.loc[train_indices].dependent_variable,
        estimator=clf,
        cv=5,
        n_jobs=1,
        verbose=2)

average_score =\
    np.array(scores).mean()

# now let's see how this compares with the actual test score
clf_all = RandomForestClassifier(n_estimators=100, random_state=1)
_X_test = df.loc[test_indices]\
            .independent_variable\
            .values\
            .reshape(-1,1)
clf_all.fit(_X_test,
            df.loc[test_indices].dependent_variable)

actual_score =\
  accuracy_score(
    clf_all.predict(_X_test),
    df.loc[test_indices].dependent_variable
  )

adversarial_result = abs(average_adversarial_score - actual_score)
print(f"... adversarial labelled validation was {adversarial_result} percent different than actual.")

cross_val_result = abs(average_score - actual_score)
print(f"... adversarial labelled validation was {cross_val_result:.2f} percent different than actual.")

# adversarial model validation gives us a better result 
# here under this data drift
--
#####################################################
--
Will continue to modify the above as needed

but inserted some reshaping logic, imbalanced learn doens't like
feature matrices of (1,), (actually, sklearn might not, since it's 
undeterminate; could be columns or rows). Need to circle abck and
verify that the test fixture still works (shoudl sicne it uses a larger
feature array, won't trigger that reshape logic; but I do stop values
from being passed down)

anwyay, so the pipeline now runs so the rest of this example
should be fairly easy (assuming it improves over the standard approach)
---
okay, am able to reproduce to where I left off on a totally new 
system so I think things are generally reproducible
I don't like how there's not enough time for the RandomizedSearchCV
to find a better solution than predicting all 1's, want to see
if I can bump the time given

yeah, tweaked it to do 200 n_iters (sample of the distribution)
and we're at 75 accuracy with much better F1 score, yay

k, i'll follow up tmmrw as I can
----
Okay there's something up with the labeller
I got the machinery all running but look

In [70]: scorer.label(_X_test)                                                                                                   
Out[70]: 
array([0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0,
       0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1,
       0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0,
       0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,
       0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,
       0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0,
       0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0,
       0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0,
       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,
       0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,
       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,
       0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0])

In [71]: sum(scorer.label(_X_test))                                                                                              
Out[71]: 101

In [72]: sum(scorer.label(_X_test))/len(_X_test)                                                                                 
Out[72]: 0.33666666666666667

but
In [71]: sum(scorer.label(_X_test))                                                                                              
Out[71]: 101

In [72]: sum(scorer.label(_X_test))/len(_X_test)                                                                                 
Out[72]: 0.33666666666666667

so i'm not sure how the labeller before this but
it should've flipped binary predictions here or we
just leave it.
--

I think I conflated somethings in the factory class
okay, so first potential failure point is the
validation could be done on already seen data but I'm 
pretty sure that i split into test, train and run on that

so then, what do i validate on? I wonder if I'm seeing
order effects? Sample should account for that? 
Maybe I don't fit to whole dataset, since it's only valideted
on train split generated after data has been passed in

okay, well there's another issue where sometimes the CV best parameters
are for somethign that predicst all one class (failure);  should
be able to detect this and restart or something

re-running
I suspect part of the issue here is that the test validation results
don't reproduce

okay, wait, so I now see:

maximize_binary_validation_accuracy check score: 0.69 (keep as is)
Validation Accuracy: 0.69
              precision    recall  f1-score   support

           0       0.93      0.60      0.73       103
           1       0.51      0.89      0.65        47

    accuracy                           0.69       150
   macro avg       0.72      0.75      0.69       150
weighted avg       0.79      0.69      0.70       150


In [99]: sum(scorer.label(_X_test))/len(_X_test)                                                                                 
Out[99]: 0.88
--
soooooo
`validation_mask = scorer.label(_X_test) == 1`
may not be the maskt that I'm looking for; it's all true and false
no that works out, from what i can tell
-
so I can directly reproduce with
```
In [126]: validation_mask = scorer.label(_X) == 1                                                                                

In [127]: actual_score2 =\ 
     ...:   accuracy_score( 
     ...:     clf_all.predict(_X[validation_mask]), 
     ...:     df.loc[train_indices].dependent_variable[validation_mask] 
     ...:   )                                                                                                                    

In [128]: actual_score2                                                                                                          
Out[128]: 1.0
```

okay, so it's pretty clear that since clf_all was trained on
_X, we'd see a 1.0 result. 
So this suggests that I needt to make yet another hold out
from _X, train on not that, but test on that
todo ^
Alternatively, I could sample from test 
actually I think that's what's going on here, i confused myself
by the concurrent test/train sets;

k, wait, no i need hold out part of _X since i'm fitting on it, of course 

I'll try to pick this up tmmrw before my meeting
-

Okay, real quick
scorer.grade, in this run, is preforming well on hold data

maximize_binary_validation_accuracy check score: 0.69 (keep as is)
Validation Accuracy: 0.69
              precision    recall  f1-score   support

           0       0.93      0.60      0.73       103
           1       0.51      0.89      0.65        47

    accuracy                           0.69       150
   macro avg       0.72      0.75      0.69       150
weighted avg       0.79      0.69      0.70       150


In [99]: sum(scorer.label(_X_test))/len(_X_test)                                                                                 
Out[99]: 0.88

So, I really should see a difference when cross validating it
-
okay, so with the labeller i'm seeing: In [133]: average_adversarial_score                                                                                              
Out[133]: 0.8176563691838291

w/o the labeller i'm seeing

In [135]: average_score                                                                                                          
Out[135]: 0.922823103219552

and

# on the hold out data I actually get (the real McCoy)
actual_score2 =\
       accuracy_score( 
         clf_all.predict(_X_test), 
         df.loc[test_indices].dependent_variable
       )
     
actual_score2
0.7

so adversarial labelling helps approach the actual hold out score

question now is if more folds will approach 0.7 or not.
It probably approaches the actual hold out score with a 
margin of error defined by labelling accuracy, would
be curious to know the asymptomic behvior

okay, let's modiy the README and clean this up later.
--

Okay i was able to reproduce the above, it's pretty amazing.
I switched the parameter search to use grid search since it was 
too unreliable to use random search. 

I still needt o circle back and make sure/get the fixture example
working but I'll do that this weekend or Friday.
I also would like to update the pypi page but that's a bit far on my todo
list right now.
--
12/23

Starting back up on this after finishing out the semester
need to make sure it first runs as expected, then copy over the extra tests
and debug/fix until they work as well. Then I can move on to the 
other repo.
Okay, pytest runs fine. Now for 
--
okay, so I got something working end to end, still need to
double check that it's doing what I think it's doing.

Unfortunately it looks like I'm scoring the same as before 
even though it's easier to identify the out of sample instances.

Given that this is a random forest, which does horribly on timeseries,
this easily could mis lead me to think the adverarial classifier doesn't work
in this case.
---
```
        if return_estimator:
            fitted_estimators = zipped_scores.pop()
>       test_scores, fit_times, score_times = zipped_scores
E       ValueError: not enough values to unpack (expected 3, got 0)

../../.local/share/virtualenvs/adversarial_labeller-OVgwaBZX/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:240: ValueError
```
 okay so i corrected the label flipping and now have the error above
  ... not getting any predicted values (it may not be returning any test labels)

need to look into this later in the week.
--
12/24
- happy christmas eve!
okay, snuck in some time, i re created the cross validator, it was
a generator that was exhausted, which probably caused the no score issue above.
re-running i get this:

tests/test_timeseries_labelled_model_validation.py 	 cross validation score: 0.8314976447141837
	 holdout_score: 0.9420708235408883
Fitting 2 folds for each of 800 candidates, totalling 1600 fits
[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.
[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    5.3s
[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   36.2s
[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:  1.4min
[Parallel(n_jobs=-1)]: Done 792 tasks      | elapsed:  2.3min
[Parallel(n_jobs=-1)]: Done 1600 out of 1600 | elapsed:  2.7min finished
maximize_binary_validation_accuracy check score: 0.93
Validation Accuracy: 0.93
              precision    recall  f1-score   support

           0       0.99      0.93      0.96       127
           1       0.65      0.94      0.77        18

    accuracy                           0.93       145
   macro avg       0.82      0.94      0.87       145
weighted avg       0.95      0.93      0.94       145

	 adversarial cross validation score: -1.8221758553917274e+21
.
--
the cross validation R^2 is basically 0, the same as predicting the mean.
--
tests/test_timeseries_labelled_model_validation.py 	 cross validation score: 0.8314976447141837
Fitting 2 folds for each of 800 candidates, totalling 1600 fits
[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.
[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    8.9s
[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   37.1s
[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:  1.3min
[Parallel(n_jobs=-1)]: Done 792 tasks      | elapsed:  2.2min
[Parallel(n_jobs=-1)]: Done 1532 tasks      | elapsed:  2.5min
[Parallel(n_jobs=-1)]: Done 1593 out of 1600 | elapsed:  2.6min remaining:    0.7s
[Parallel(n_jobs=-1)]: Done 1600 out of 1600 | elapsed:  2.6min finished
maximize_binary_validation_accuracy check score: 0.88
Validation Accuracy: 0.88
              precision    recall  f1-score   support

           0       0.88      1.00      0.93       127
           1       0.00      0.00      0.00        18

    accuracy                           0.88       145
   macro avg       0.44      0.50      0.47       145
weighted avg       0.77      0.88      0.82       145

	 adversarial cross validation score: 0.0
	 actual hold out score: 0.9420708235408883
.

--
okay, so i reorded some of the print out, double checked that everything is using
the same metric. Let me look at the test labels that are coming out; 
it's possible that requiring label = 1 is too stringent of a test.

Another thing I'm noticing is that the labeller is outputing different accuracy
per run, which it shouldn't since it does a grid run. EDIT - the last run matched the above,
not sure where the randomness is coming from.

An inspection of the masks all reveal False :(
--
okay, so i investigated the predicited probabilities and very few greater than 50%
for the test label. Additionally, since this is a timeseries, the 
folds are not iid and it makes sense that not much really matches.

I'm thinking I should use proba threshold
-
added threshold. re-running
okay, so I'm seeing -0.28 r^2 at a 0.59 threshold.
Seeing if I can reproduce the same result with no grader
by setting the threshold to 0.1 (should allow every instance to be used for
cross validation)
-
okay, this is what I'm seeing
threshold # 0.1, 0.2,  .4    0.59
r^2 score # 0.83 0.83 0.00  -0.23

I should re-run the original (first test), make sure it's
as expected, then more thoroughly consider the problem. I
think there's is difference between data shift and timeseries
whereby testing on folds isn't quite right or i'm learning
to classify samples wrong?
-
The problem is, really, the labelling distribution must be somehow 
different than the actual test distribution and/or the splits
tested are different (since we're pickign subsets)
--
(Pdb) X_test.groupby([X_test.index.day, X_test.index.hour]).mean()
                        Open       High        Low        Volume  OpenInt
Datetime Datetime                                                        
5        20        81.393333  81.402222  81.388889    739.666667      0.0
         21        81.357500  81.374167  81.346667   1758.666667      0.0
         22        81.310000  81.360000  81.300000   4737.000000      0.0
6        15        81.409000  81.419000  81.398000  22654.000000      0.0
         16        81.403667  81.410000  81.393250  11187.166667      0.0
         17        81.412500  81.414250  81.404583   8725.166667      0.0
         18        81.414583  81.418750  81.408750   5302.666667      0.0
         19        81.418333  81.419167  81.413750   9378.916667      0.0
         20        81.440417  81.446667  81.434583  12191.250000      0.0
         21        81.475833  81.489167  81.465425   8092.250000      0.0
         22        81.500000  81.500000  81.460000   4983.000000      0.0

so here we see that the test Volumne is much higher, compare to train
which is all over the place (including values a mag. higher and just as low)

Let's look at the y value,
import scipy
from scipy.stats import ks_2samp
ks_2samp(y_train, y_test)
Ks_2sampResult(statistic=0.9261751152073733, pvalue=1.3322676295501878e-15)

So, then, uh, the labeller should be able to select instances such that
the Ks_2sample result has a p-value > 0.05
-
k = scorer.label(X_train)
(Pdb) ks_2samp(y_train[k>0.50], y_test)
Ks_2sampResult(statistic=0.5542857142857143, pvalue=0.0004939356767378333)
^ using a lower k> threshold results in a smaller and small pvalue
but that's still a pretty small pvalue.

Also, looking at the predicted_proba over X_train, we see it grow to 0.55 test label
more as it gets closer to the end of the timeseries.
-
(Pdb) m = scorer.label(X_test)
(Pdb) m
array([0.55172912, 0.55172912, 0.55172912, 0.55172912, 0.55172912,
       0.55172912, 0.55172912, 0.55172912, 0.55172912, 0.55172912,
       0.55172912, 0.55172912, 0.35510114, 0.35510114, 0.55172912,
       0.55172912, 0.55172912, 0.55172912, 0.55172912, 0.55172912,
       0.37444049, 0.37444049, 0.55172912, 0.55172912, 0.55172912,
       0.55172912, 0.55172912, 0.55172912, 0.55172912, 0.55172912,
       0.55172912, 0.55172912, 0.55172912, 0.55172912, 0.55172912,
       0.55172912, 0.55172912, 0.55172912, 0.55172912, 0.55172912,
       0.55172912, 0.55172912, 0.55172912, 0.55172912, 0.55172912,
       0.55172912, 0.55172912, 0.55172912, 0.55172912, 0.55172912,
       0.55172912, 0.55172912, 0.55172912, 0.55172912, 0.55172912,
       0.55172912, 0.55172912, 0.55172912, 0.55172912, 0.55172912,
       0.55172912, 0.55172912, 0.55172912, 0.55172912, 0.55172912,
       0.55172912, 0.55172912, 0.55172912, 0.55172912, 0.55172912,
       0.55172912, 0.55172912, 0.55172912, 0.55172912, 0.55172912,
       0.55172912, 0.55172912, 0.55172912, 0.55172912, 0.55172912,
       0.55172912, 0.55172912, 0.55172912, 0.55172912, 0.55172912,
       0.55172912, 0.55172912, 0.55172912, 0.55172912, 0.55172912,
       0.55172912, 0.55172912, 0.55172912, 0.55172912, 0.55172912,
       0.55172912, 0.55172912, 0.55172912, 0.55172912, 0.55172912])
-
^ this actually might be useful to set a threshold (since we know what
the test data is)

okay, so now we should uh predict on that
-
(Pdb) y_train[k>0.55]
array([81.365, 81.36 , 81.38 , 81.37 , 81.34 , 81.35 , 81.48 , 81.36 ,
       81.35 , 81.37 , 81.36 , 81.42 , 81.43 , 81.42 ])
(Pdb) scoring_metric
<function r2_score at 0x10fa7c7a0>
(Pdb) scoring_metric(y_train[k>0.55], clf.predict(X_train[k>0.55])
*** SyntaxError: unexpected EOF while parsing
(Pdb) scoring_metric(y_train[k>0.55], clf.predict(X_train[k>0.55]))
*** sklearn.exceptions.NotFittedError: This LinearRegression instance is not fitted yet. Call 'fit' with appropriate arguments before using this method.
(Pdb) clf = LinearRegression(fit_intercept=True, normalize=False, n_jobs=-1)
(Pdb) clf.fit(X_train[k<0.55], y_train[k<0.55])
LinearRegression(copy_X=True, fit_intercept=True, n_jobs=-1, normalize=False)
(Pdb) scoring_metric(y_train[k>0.55], clf.predict(X_train[k>0.55]))
-0.5607937416014719

Okkkkkkkaaaay, hmm. so the distributions are different.
---
(Pdb) scoring_metric(y_train[k>0.55], clf.predict(X_train[k>0.55]))
-0.5607937416014719
(Pdb) scoring_metric(y_test, clf.predict(X_test))
0.9504252213996738
(Pdb) 
---
and,
(Pdb) ks_2samp(y_train[k>0.55], y_train[k<=0.55])
Ks_2sampResult(statistic=0.7845433255269321, pvalue=1.8978276727921184e-09)
-
The other thing to consider is how out of sample testing is or is not
appropriate for time series testing; but 